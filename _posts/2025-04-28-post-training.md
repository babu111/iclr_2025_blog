---
layout: distill
title: Post-training will prevail
description: Your blog post's abstract.
date: 2025-04-28
future: true
htmlwidgets: true
hidden: false

# Anonymize when submitting
authors:
  - name: Anonymous

# authors:
#   - name: Albert Einstein
#     url: "https://en.wikipedia.org/wiki/Albert_Einstein"
#     affiliations:
#       name: IAS, Princeton
#   - name: Boris Podolsky
#     url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
#     affiliations:
#       name: IAS, Princeton
#   - name: Nathan Rosen
#     url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
#     affiliations:
#       name: IAS, Princeton

# must be the exact same name as your blogpost
bibliography: 2025-04-28-distill-example.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Equations
  - name: Images and Figures
    subsections:
    - name: Interactive Figures
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Diagrams
  - name: Tweets
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.

##### Introduction

Large language models (LLMs) have achieved remarkable performance across a variety of tasks, such as language comprehension, code generation, and solving mathematical problems~\cite{brown2020language, chowdhery2022palm, kaplan2020scaling, scao2022bloom, touvron2023llama, zeng2022glm, team2023gemini, openai2023gpt}. Substantial increase in model parameters~\cite{zhang2022opt}, computational resources, and the size of training datasets led to notable results signifying intelligence. For example, GPT-4-Turbo with multi-agent prompting scored 76.73\%~\cite{lei2024macm} on MATH~\cite{hendrycks2021measuring}, and DeepSeek-Prover~\cite{xin2024deepseek} scored 46.3\% with 64 attempts on miniF2F~\cite{zheng2021minif2f}.

recover from missteps



##### Post-training is lifelong

pretraining is similar to genetic inhere and learning in school

lifelong learning is not pretraining, but a part of post-training

turtle babies climb up to the shore immediately after they are hatched

in tasks where no data can be found



##### customer-focused product will succeed

If we believe that scaling the right thing will lead us to AGI.

It's takes longer to scale to the next exponentially larger cluster.





##### How do models know when they are right?

