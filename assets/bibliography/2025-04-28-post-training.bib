@article{greenwade93,
    author  = "George D. Greenwade",
    title   = "The {C}omprehensive {T}ex {A}rchive {N}etwork ({CTAN})",
    year    = "1993",
    journal = "TUGBoat",
    volume  = "14",
    number  = "3",
    pages   = "342--351"
}
@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}
@article{sorscher2022beyond,
  title={Beyond neural scaling laws: beating power law scaling via data pruning},
  author={Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={19523--19536},
  year={2022}
}

@inproceedings{cazenavette2022dataset,
  title={Dataset distillation by matching training trajectories},
  author={Cazenavette, George and Wang, Tongzhou and Torralba, Antonio and Efros, Alexei A and Zhu, Jun-Yan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4750--4759},
  year={2022}
}

@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{wang2018dataset,
  title={Dataset distillation},
  author={Wang, Tongzhou and Zhu, Jun-Yan and Torralba, Antonio and Efros, Alexei A},
  journal={arXiv preprint arXiv:1811.10959},
  year={2018}
}

@inproceedings{guo2022deepcore,
  title={Deepcore: A comprehensive library for coreset selection in deep learning},
  author={Guo, Chengcheng and Zhao, Bo and Bai, Yanbing},
  booktitle={International Conference on Database and Expert Systems Applications},
  pages={181--195},
  year={2022},
  organization={Springer}
}

@article{nemhauser1978analysis,
  title={An analysis of approximations for maximizing submodular set functions—I},
  author={Nemhauser, George L and Wolsey, Laurence A and Fisher, Marshall L},
  journal={Mathematical programming},
  volume={14},
  pages={265--294},
  year={1978},
  publisher={Springer}
}

@book{fisher1978analysis,
  title={An analysis of approximations for maximizing submodular set functions—II},
  author={Fisher, Marshall L and Nemhauser, George L and Wolsey, Laurence A},
  year={1978},
  publisher={Springer}
}

@inproceedings{lin2009select,
  title={How to select a good training-data subset for transcription: submodular active selection for sequences.},
  author={Lin, Hui and Bilmes, Jeff A},
  booktitle={Interspeech},
  pages={2859--2862},
  year={2009}
}
@article{su2022selective,
  title={Selective annotation makes language models better few-shot learners},
  author={Su, Hongjin and Kasai, Jungo and Wu, Chen Henry and Shi, Weijia and Wang, Tianlu and Xin, Jiayi and Zhang, Rui and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A and others},
  journal={arXiv preprint arXiv:2209.01975},
  year={2022}
}
@inproceedings{zhao2023dataset,
  title={Dataset condensation with distribution matching},
  author={Zhao, Bo and Bilen, Hakan},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={6514--6523},
  year={2023}
}
@inproceedings{cui2023scaling,
  title={Scaling up dataset distillation to imagenet-1k with constant memory},
  author={Cui, Justin and Wang, Ruochen and Si, Si and Hsieh, Cho-Jui},
  booktitle={International Conference on Machine Learning},
  pages={6565--6590},
  year={2023},
  organization={PMLR}
}
@inproceedings{sucholutsky2021soft,
  title={Soft-label dataset distillation and text dataset distillation},
  author={Sucholutsky, Ilia and Schonlau, Matthias},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}
@inproceedings{zhao2021dataset,
  title={Dataset condensation with differentiable siamese augmentation},
  author={Zhao, Bo and Bilen, Hakan},
  booktitle={International Conference on Machine Learning},
  pages={12674--12685},
  year={2021},
  organization={PMLR}
}
@inproceedings{zhao2020dataset,
  title={Dataset Condensation with Gradient Matching},
  author={Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{bohdal2020flexible,
  title={Flexible Dataset Distillation: Learn Labels Instead of Images},
  author={Bohdal, Ondrej and Yang, Yongxin and Hospedales, Timothy M},
  booktitle={4th Workshop on Meta-Learning at NeurIPS 2020},
  year={2020}
}
@inproceedings{wang2022cafe,
  title={Cafe: Learning to condense dataset by aligning features},
  author={Wang, Kai and Zhao, Bo and Peng, Xiangyu and Zhu, Zheng and Yang, Shuo and Wang, Shuo and Huang, Guan and Bilen, Hakan and Wang, Xinchao and You, Yang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12196--12205},
  year={2022}
}
@inproceedings{coleman2020selection,
  title={Selection via Proxy: Efficient Data Selection for Deep Learning},
  author={Coleman, C and Yeh, C and Mussmann, S and Mirzasoleiman, B and Bailis, P and Liang, P and Leskovec, J and Zaharia, M},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}
@inproceedings{welling2009herding,
  title={Herding dynamical weights to learn},
  author={Welling, Max},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={1121--1128},
  year={2009}
}
@book{farahani2009facility,
  title={Facility location: concepts, models, algorithms and case studies},
  author={Farahani, Reza Zanjirani and Hekmatfar, Masoud},
  year={2009},
  publisher={Springer Science \& Business Media}
}
@article{sener2017active,
  title={Active learning for convolutional neural networks: A core-set approach},
  author={Sener, Ozan and Savarese, Silvio},
  journal={arXiv preprint arXiv:1708.00489},
  year={2017}
}
@inproceedings{park2022active,
  title={Active learning is a strong baseline for data subset selection},
  author={Park, Dongmin and Papailiopoulos, Dimitris and Lee, Kangwook},
  booktitle={Has it Trained Yet? NeurIPS 2022 Workshop},
  year={2022}
}
@article{iyer2013submodular,
  title={Submodular optimization with submodular cover and submodular knapsack constraints},
  author={Iyer, Rishabh K and Bilmes, Jeff A},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{bachem2017practical,
  title={Practical coreset constructions for machine learning},
  author={Bachem, Olivier and Lucic, Mario and Krause, Andreas},
  journal={arXiv preprint arXiv:1703.06476},
  year={2017}
}
@article{mirzasoleiman2013distributed,
  title={Distributed submodular maximization: Identifying representative elements in massive data},
  author={Mirzasoleiman, Baharan and Karbasi, Amin and Sarkar, Rik and Krause, Andreas},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}
@inproceedings{mirzasoleiman2015lazier,
  title={Lazier than lazy greedy},
  author={Mirzasoleiman, Baharan and Badanidiyuru, Ashwinkumar and Karbasi, Amin and Vondr{\'a}k, Jan and Krause, Andreas},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={29},
  number={1},
  year={2015}
}
@article{paul2021deep,
  title={Deep learning on a data diet: Finding important examples early in training},
  author={Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20596--20607},
  year={2021}
}
@inproceedings{mirzasoleiman2020coresets,
  title={Coresets for data-efficient training of machine learning models},
  author={Mirzasoleiman, Baharan and Bilmes, Jeff and Leskovec, Jure},
  booktitle={International Conference on Machine Learning},
  pages={6950--6960},
  year={2020},
  organization={PMLR}
}
@article{yang2023towards,
  title={Towards Sustainable Learning: Coresets for Data-efficient Deep Learning},
  author={Yang, Yu and Kang, Hao and Mirzasoleiman, Baharan},
  journal={arXiv preprint arXiv:2306.01244},
  year={2023}
}
@inproceedings{kim2022dataset,
  title={Dataset condensation via efficient synthetic-data parameterization},
  author={Kim, Jang-Hyun and Kim, Jinuk and Oh, Seong Joon and Yun, Sangdoo and Song, Hwanjun and Jeong, Joonhyun and Ha, Jung-Woo and Song, Hyun Oh},
  booktitle={International Conference on Machine Learning},
  pages={11102--11118},
  year={2022},
  organization={PMLR}
}
@article{mirzasoleiman2016distributed,
  title={Distributed submodular maximization},
  author={Mirzasoleiman, Baharan and Karbasi, Amin and Sarkar, Rik and Krause, Andreas},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={8330--8373},
  year={2016},
  publisher={JMLR. org}
}
@article{bilmes2022submodularity,
  title={Submodularity in machine learning and artificial intelligence},
  author={Bilmes, Jeff},
  journal={arXiv preprint arXiv:2202.00132},
  year={2022}
}
@article{li2023alpacaeval,
  title={Alpacaeval: An automatic evaluator of instruction-following models},
  author={Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={GitHub repository},
  year={2023}
}
@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{liu2023alignbench,
  title={Alignbench: Benchmarking chinese alignment of large language models},
  author={Liu, Xiao and Lei, Xuanyu and Wang, Shengyuan and Huang, Yue and Feng, Zhuoer and Wen, Bosi and Cheng, Jiale and Ke, Pei and Xu, Yifan and Tam, Weng Lam and others},
  journal={arXiv preprint arXiv:2311.18743},
  year={2023}
}
@article{wolsey1982analysis,
  title={An analysis of the greedy algorithm for the submodular set covering problem},
  author={Wolsey, Laurence A},
  journal={Combinatorica},
  volume={2},
  number={4},
  pages={385--393},
  year={1982},
  publisher={Springer}
}
@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}
@article{mirzasoleiman2015distributed,
  title={Distributed submodular cover: Succinctly summarizing massive data},
  author={Mirzasoleiman, Baharan and Karbasi, Amin and Badanidiyuru, Ashwinkumar and Krause, Andreas},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}
@inproceedings{feige1996threshold,
  title={A threshold of ln n for approximating set cover (preliminary version)},
  author={Feige, Uriel},
  booktitle={Proceedings of the twenty-eighth annual ACM symposium on Theory of computing},
  pages={314--318},
  year={1996}
}
@book{leiserson1994introduction,
  title={Introduction to algorithms},
  author={Leiserson, Charles Eric and Rivest, Ronald L and Cormen, Thomas H and Stein, Clifford},
  volume={3},
  year={1994},
  publisher={MIT press Cambridge, MA, USA}
}
@article{khuller1999budgeted,
  title={The budgeted maximum coverage problem},
  author={Khuller, Samir and Moss, Anna and Naor, Joseph Seffi},
  journal={Information processing letters},
  volume={70},
  number={1},
  pages={39--45},
  year={1999},
  publisher={Elsevier}
}
@article{vohra1993probabilistic,
  title={A probabilistic analysis of the maximal covering location problem},
  author={Vohra, Rakesh V and Hall, Nicholas G},
  journal={Discrete Applied Mathematics},
  volume={43},
  number={2},
  pages={175--183},
  year={1993},
  publisher={Elsevier}
}
@misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{wei2021finetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}
@article{kopf2023openassistant,
  title={OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={arXiv preprint arXiv:2304.07327},
  year={2023}
}

@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}

@inproceedings{du2022glm,
  title={GLM: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}
@inproceedings{angelova2005pruning,
  title={Pruning training sets for learning of object categories},
  author={Angelova, Anelia and Abu-Mostafam, Yaser and Perona, Pietro},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  volume={1},
  pages={494--501},
  year={2005},
  organization={IEEE}
}

@phdthesis{angelova2004data,
  title={Data pruning},
  author={Angelova, Anelia Nedelcheva},
  year={2004},
  school={California Institute of Technology}
}

@article{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@inproceedings{wang2022super,
  title={Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{iyer2022opt,
  title={Opt-iml: Scaling language model instruction meta learning through the lens of generalization},
  author={Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
  journal={arXiv preprint arXiv:2212.12017},
  year={2022}
}
@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}
@article{johnson2019billion,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}
@misc{rapidsai,
  title={Rapidsai/raft: RAFT contains fundamental widely-used algorithms and primitives for data science, Graph and machine learning.},
  url={https://github.com/rapidsai/raft},
  journal={GitHub},
  publisher={Nvidia RAPIDS},
  author={Rapidsai},
  year={2022}
}
@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: an Open Dataset for Training Large Language Models},
  month = October,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}
@article{tirumala2024d4,
  title={D4: Improving llm pretraining via document de-duplication and diversification},
  author={Tirumala, Kushal and Simig, Daniel and Aghajanyan, Armen and Morcos, Ari},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}
@article{abbas2023semdedup,
  title={Semdedup: Data-efficient learning at web-scale through semantic deduplication},
  author={Abbas, Amro and Tirumala, Kushal and Simig, D{\'a}niel and Ganguli, Surya and Morcos, Ari S},
  journal={arXiv preprint arXiv:2303.09540},
  year={2023}
}
@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{chen2023alpagasus,
  title={Alpagasus: Training a better alpaca with fewer data},
  author={Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others},
  journal={arXiv preprint arXiv:2307.08701},
  year={2023}
}
@article{bukharin2023data,
  title={Data diversity matters for robust instruction tuning},
  author={Bukharin, Alexander and Zhao, Tuo},
  journal={arXiv preprint arXiv:2311.14736},
  year={2023}
}
@article{tan2024data,
  title={Data pruning via moving-one-sample-out},
  author={Tan, Haoru and Wu, Sitong and Du, Fei and Chen, Yukang and Wang, Zhibin and Wang, Fan and Qi, Xiaojuan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{gudibande2023false,
  title={The false promise of imitating proprietary llms},
  author={Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
  journal={arXiv preprint arXiv:2305.15717},
  year={2023}
}
@article{dong2023steerlm,
  title={Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf},
  author={Dong, Yi and Wang, Zhilin and Sreedhar, Makesh Narsimhan and Wu, Xianchao and Kuchaiev, Oleksii},
  journal={arXiv preprint arXiv:2310.05344},
  year={2023}
}
@article{xia2024less,
  title={Less: Selecting influential data for targeted instruction tuning},
  author={Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi},
  journal={arXiv preprint arXiv:2402.04333},
  year={2024}
}
@article{wu2023self,
  title={Self-evolved diverse data sampling for efficient instruction tuning},
  author={Wu, Shengguang and Lu, Keming and Xu, Benfeng and Lin, Junyang and Su, Qi and Zhou, Chang},
  journal={arXiv preprint arXiv:2311.08182},
  year={2023}
}
@inproceedings{nohyun2022data,
  title={Data valuation without training of a model},
  author={Nohyun, Ki and Choi, Hoyong and Chung, Hye Won},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@article{li2023quantity,
  title={From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning},
  author={Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing},
  journal={arXiv preprint arXiv:2308.12032},
  year={2023}
}
@article{lin2024data,
  title={Data-efficient Fine-tuning for LLM-based Recommendation},
  author={Lin, Xinyu and Wang, Wenjie and Li, Yongqi and Yang, Shuo and Feng, Fuli and Wei, Yinwei and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2401.17197},
  year={2024}
}
@article{sharma2024text,
  title={Text Quality-Based Pruning for Efficient Training of Language Models},
  author={Sharma, Vasu and Padthe, Karthik and Ardalani, Newsha and Tirumala, Kushal and Howes, Russell and Xu, Hu and Huang, Po-Yao and Li, Shang-Wen and Aghajanyan, Armen and Ghosh, Gargi},
  journal={arXiv preprint arXiv:2405.01582},
  year={2024}
}

@article{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}
@article{chen2024alphamath,
  title={AlphaMath Almost Zero: process Supervision without process},
  author={Chen, Guoxin and Liao, Minpeng and Li, Chengxi and Fan, Kai},
  journal={arXiv preprint arXiv:2405.03553},
  year={2024}
}
@article{zhang2024rest,
  title={ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search},
  author={Zhang, Dan and Zhoubian, Sining and Yue, Yisong and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2406.03816},
  year={2024}
}
@article{tian2024toward,
  title={Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing},
  author={Tian, Ye and Peng, Baolin and Song, Linfeng and Jin, Lifeng and Yu, Dian and Mi, Haitao and Yu, Dong},
  journal={arXiv preprint arXiv:2404.12253},
  year={2024}
}
@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{hwang2024transformerfam,
  title={TransformerFAM: Feedback attention is working memory},
  author={Hwang, Dongseong and Wang, Weiran and Huo, Zhuoyuan and Sim, Khe Chai and Mengibar, Pedro Moreno},
  journal={arXiv preprint arXiv:2404.09173},
  year={2024}
}
@article{lightman2023let,
  title={Let's Verify Step by Step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}
@article{chen2024self,
  title={Self-play fine-tuning converts weak language models to strong language models},
  author={Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:2401.01335},
  year={2024}
}
@article{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{liu2024self,
  title={Self-Corrected Multimodal Large Language Model for End-to-End Robot Manipulation},
  author={Liu, Jiaming and Li, Chenxuan and Wang, Guanqun and Lee, Lily and Zhou, Kaichen and Chen, Sixiang and Xiong, Chuyan and Ge, Jiaxin and Zhang, Renrui and Zhang, Shanghang},
  journal={arXiv preprint arXiv:2405.17418},
  year={2024}
}
@article{gulcehre2023reinforced,
  title={Reinforced self-training (rest) for language modeling},
  author={Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
  journal={arXiv preprint arXiv:2308.08998},
  year={2023}
}
@article{zhou2023webarena,
  title={Webarena: A realistic web environment for building autonomous agents},
  author={Zhou, Shuyan and Xu, Frank F and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Bisk, Yonatan and Fried, Daniel and Alon, Uri and others},
  journal={arXiv preprint arXiv:2307.13854},
  year={2023}
}
@article{trinh2024solving,
  title={Solving olympiad geometry without human demonstrations},
  author={Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and Luong, Thang},
  journal={Nature},
  volume={625},
  number={7995},
  pages={476--482},
  year={2024},
  publisher={Nature Publishing Group}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{valmeekam2022large,
  title={Large language models still can't plan (a benchmark for llms on planning and reasoning about change)},
  author={Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2206.10498},
  year={2022}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}
@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{yuan2024advancing,
  title={Advancing LLM Reasoning Generalists with Preference Trees},
  author={Yuan, Lifan and Cui, Ganqu and Wang, Hanbin and Ding, Ning and Wang, Xingyao and Deng, Jia and Shan, Boji and Chen, Huimin and Xie, Ruobing and Lin, Yankai and others},
  journal={arXiv preprint arXiv:2404.02078},
  year={2024}
}
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}
@inproceedings{clark2015training,
  title={Training deep convolutional neural networks to play go},
  author={Clark, Christopher and Storkey, Amos},
  booktitle={International conference on machine learning},
  pages={1766--1774},
  year={2015},
  organization={PMLR}
}
@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}
@article{openai2023gpt,
  title={GPT-4 technical report},
  author={OpenAI, R},
  journal={arXiv},
  pages={2303--08774},
  year={2023}
}
@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{besta2024graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={17682--17690},
  year={2024}
}
@article{ding2023everything,
  title={Everything of thoughts: Defying the law of penrose triangle for thought generation},
  author={Ding, Ruomeng and Zhang, Chaoyun and Wang, Lu and Xu, Yong and Ma, Minghua and Zhang, Wei and Qin, Si and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2311.04254},
  year={2023}
}
@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}
@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}
@article{stechly2024self,
  title={On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks},
  author={Stechly, Kaya and Valmeekam, Karthik and Kambhampati, Subbarao},
  journal={arXiv preprint arXiv:2402.08115},
  year={2024}
}
@article{saunders2022self,
  title={Self-critiquing models for assisting human evaluators},
  author={Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  journal={arXiv preprint arXiv:2206.05802},
  year={2022}
}

@article{huang2023large,
  title={Large language models cannot self-correct reasoning yet},
  author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.01798},
  year={2023}
}
@article{hong2023closer,
  title={A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning},
  author={Hong, Ruixin and Zhang, Hongming and Pang, Xinyu and Yu, Dong and Zhang, Changshui},
  journal={arXiv preprint arXiv:2311.07954},
  year={2023}
}
@article{silver2017mastering,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}
@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}
@article{Ramamurthy2022IsRL,
  title={Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization},
  author={Rajkumar Ramamurthy and Prithviraj Ammanabrolu and Kiant{\'e} Brantley and Jack Hessel and Rafet Sifa and Christian Bauckhage and Hannaneh Hajishirzi and Yejin Choi},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.01241},
  url={https://api.semanticscholar.org/CorpusID:252693405}
}
@article{lei2024macm,
  title={MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems},
  author={Lei, Bin},
  journal={arXiv preprint arXiv:2404.04735},
  year={2024}
}
@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}
@article{zheng2021minif2f,
  title={MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics},
  author={Zheng, Kunhao and Han, Jesse Michael and Polu, Stanislas},
  journal={arXiv preprint arXiv:2109.00110},
  year={2021}
}
@article{xin2024deepseek,
  title={DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data},
  author={Xin, Huajian and Guo, Daya and Shao, Zhihong and Ren, Zhizhou and Zhu, Qihao and Liu, Bo and Ruan, Chong and Li, Wenda and Liang, Xiaodan},
  journal={arXiv preprint arXiv:2405.14333},
  year={2024}
}
@article{yuan2024self,
  title={Self-rewarding language models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  journal={arXiv preprint arXiv:2401.10020},
  year={2024}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}
@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}
@article{prakash2024fine,
  title={Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking},
  author={Prakash, Nikhil and Shaham, Tamar Rott and Haklay, Tal and Belinkov, Yonatan and Bau, David},
  journal={arXiv preprint arXiv:2402.14811},
  year={2024}
}
@article{liu2023ring,
  title={Ring attention with blockwise transformers for near-infinite context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}
@inproceedings{riedmiller2018learning,
  title={Learning by playing solving sparse reward tasks from scratch},
  author={Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Wiele, Tom and Mnih, Vlad and Heess, Nicolas and Springenberg, Jost Tobias},
  booktitle={International conference on machine learning},
  pages={4344--4353},
  year={2018},
  organization={PMLR}
}
@article{li2024reason,
  title={Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Understanding},
  author={Li, Yanda and Wang, Dixuan and Liang, Jiaqing and Jiang, Guochao and He, Qianyu and Xiao, Yanghua and Yang, Deqing},
  journal={arXiv preprint arXiv:2404.04293},
  year={2024}
}
@article{an2023learning,
  title={Learning from mistakes makes llm better reasoner},
  author={An, Shengnan and Ma, Zexiong and Lin, Zeqi and Zheng, Nanning and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2310.20689},
  year={2023}
}
@article{tong2024can,
  title={Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning},
  author={Tong, Yongqi and Li, Dawei and Wang, Sizhe and Wang, Yujia and Teng, Fei and Shang, Jingbo},
  journal={arXiv preprint arXiv:2403.20046},
  year={2024}
}
@inproceedings{wang2023learning,
  title={Learning from mistakes via cooperative study assistant for large language models},
  author={Wang, Danqing and Li, Lei},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={10667--10685},
  year={2023}
}
@inproceedings{garg2022learning,
  title={Learning from mistakes--a framework for neural architecture search},
  author={Garg, Bhanu and Zhang, Li and Sridhara, Pradyumna and Hosseini, Ramtin and Xing, Eric and Xie, Pengtao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={9},
  pages={10184--10192},
  year={2022}
}
@article{zhang2024context,
  title={In-Context Principle Learning from Mistakes},
  author={Zhang, Tianjun and Madaan, Aman and Gao, Luyu and Zheng, Steven and Mishra, Swaroop and Yang, Yiming and Tandon, Niket and Alon, Uri},
  journal={arXiv preprint arXiv:2402.05403},
  year={2024}
}
@article{paruchuri2024odds,
  title={What Are the Odds? Language Models Are Capable of Probabilistic Reasoning},
  author={Paruchuri, Akshay and Garrison, Jake and Liao, Shun and Hernandez, John and Sunshine, Jacob and Althoff, Tim and Liu, Xin and McDuff, Daniel},
  journal={arXiv preprint arXiv:2406.12830},
  year={2024}
}
@article{zhang2024accessing,
  title={Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B},
  author={Zhang, Di and Li, Jiatong and Huang, Xiaoshui and Zhou, Dongzhan and Li, Yuqiang and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2406.07394},
  year={2024}
}
@article{xie2024exploratory,
  title={Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF},
  author={Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2405.21046},
  year={2024}
}
@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@article{kong2024aligning,
  title={Aligning Large Language Models with Representation Editing: A Control Perspective},
  author={Kong, Lingkai and Wang, Haorui and Mu, Wenhao and Du, Yuanqi and Zhuang, Yuchen and Zhou, Yifei and Song, Yue and Zhang, Rongzhi and Wang, Kai and Zhang, Chao},
  journal={arXiv preprint arXiv:2406.05954},
  year={2024}
}
@article{hu2023amortizing,
  title={Amortizing intractable inference in large language models},
  author={Hu, Edward J and Jain, Moksh and Elmoznino, Eric and Kaddar, Younesse and Lajoie, Guillaume and Bengio, Yoshua and Malkin, Nikolay},
  journal={arXiv preprint arXiv:2310.04363},
  year={2023}
}
@article{qu2024recursive,
  title={Recursive Introspection: Teaching Language Model Agents How to Self-Improve},
  author={Qu, Yuxiao and Zhang, Tianjun and Garg, Naman and Kumar, Aviral},
  journal={arXiv preprint arXiv:2407.18219},
  year={2024}
}